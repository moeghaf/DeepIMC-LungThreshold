{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae5e084",
   "metadata": {},
   "source": [
    "### Utilities for training a model per channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925041fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: segmentation-models-pytorch 0.1.0\n",
      "Uninstalling segmentation-models-pytorch-0.1.0:\n",
      "  Successfully uninstalled segmentation-models-pytorch-0.1.0\n",
      "Collecting segmentation_models_pytorch==0.1.0\n",
      "  Using cached segmentation_models_pytorch-0.1.0-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from segmentation_models_pytorch==0.1.0) (0.15.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from segmentation_models_pytorch==0.1.0) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from segmentation_models_pytorch==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: munch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (4.66.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (1.26.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->segmentation_models_pytorch==0.1.0) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch->pretrainedmodels==0.7.4->segmentation_models_pytorch==0.1.0) (1.3.0)\n",
      "Installing collected packages: segmentation_models_pytorch\n",
      "Successfully installed segmentation_models_pytorch-0.1.0\n",
      "Requirement already satisfied: albumentations in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (1.11.3)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from albumentations) (4.8.1.78)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.8.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.2)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (10.1.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.5)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.9.26)\n",
      "Requirement already satisfied: packaging>=21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.16.1->albumentations) (3.1.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
      "Requirement already satisfied: scikit-image in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (1.11.3)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (3.2)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (10.1.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (2.31.5)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (2023.9.26)\n",
      "Requirement already satisfied: packaging>=21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-image) (0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Mohamed Ghafoor \n",
    "Contact: Moeghaf@gmail.com \n",
    "20/11/2023 \n",
    "\n",
    "'''\n",
    "\n",
    "# Installations \n",
    "!pip uninstall segmentation_models_pytorch --yes\n",
    "!pip install segmentation_models_pytorch==0.1.0\n",
    "!pip install -U albumentations\n",
    "!pip3 install scikit-image\n",
    "\n",
    "\n",
    "# Imports \n",
    "from skimage.util import view_as_windows, view_as_blocks\n",
    "import numpy as np \n",
    "import torch \n",
    "import segmentation_models_pytorch as smp\n",
    "import pandas as pd \n",
    "from glob import glob \n",
    "from tifffile import imread, imwrite\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu\n",
    "import os \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm \n",
    "from skimage import io, color, filters, exposure\n",
    "\n",
    "\n",
    "def min_max_normalize(image):\n",
    "    \"\"\"\n",
    "    Normalize pixel values in an image to the range [0, 1].\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized image with pixel values scaled to [0, 1].\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input image is not a NumPy array.\n",
    "\n",
    "    Notes:\n",
    "        - The normalization is performed using the minimum and maximum pixel values in the image.\n",
    "        - Formula: normalized_image = (image - min_value) / (max_value - min_value).\n",
    "        - This function assumes the input image has numerical pixel values.\n",
    "\n",
    "    Example:\n",
    "        >>> normalized_img = min_max_normalize(image_array)\n",
    "    \"\"\"\n",
    "    # Convert to array \n",
    "    image = np.array(image)\n",
    "\n",
    "    # Calculate the minimum and maximum pixel values in the image\n",
    "    min_value, max_value = np.min(image), np.max(image)\n",
    "\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image = (image - min_value) / (max_value - min_value)\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "\n",
    "def preprocess_data(target):\n",
    "    \"\"\"\n",
    "    Preprocesses input and output images normalized (0 to 1), histogram matched, \n",
    "    and split into 1x224x224 overlapping windows. \n",
    "\n",
    "    Parameters:\n",
    "        target (str): Specific target/channel/metal/protein identifier.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Six tensors containing training, validation, and testing windows for input and output images.\n",
    "\n",
    "    Example:\n",
    "        >>> x_train_patch, y_train_patch, x_val_patch, y_val_patch, x_test_patch, y_test_patch = preprocess_data(\"your_target\")\n",
    "    \"\"\"\n",
    "    # Load input and output paths\n",
    "    x_dirs = glob('/home/ec2-user/SageMaker/x/*/*' + target + '*')\n",
    "    y_dirs = glob('/home/ec2-user/SageMaker/Threshold_images_TS/*/*' + target + '*')\n",
    "\n",
    "    # Match histograms of input images to the first input\n",
    "    matched_image = imread(x_dirs[0])\n",
    "\n",
    "    # Normalize input and output images\n",
    "    x_images = [min_max_normalize(exposure.match_histograms(imread(i), matched_image)) for i in x_dirs]\n",
    "    y_images = [min_max_normalize(imread(i)) for i in y_dirs]\n",
    "\n",
    "    # Split the input images into training (4), validation (1), and testing (1)\n",
    "    x_train = torch.tensor([x_images[0], x_images[1], x_images[3], x_images[5]])\n",
    "    x_val = torch.tensor(x_images[4]).unsqueeze(0)\n",
    "    x_test = torch.tensor(x_images[2]).unsqueeze(0)\n",
    "\n",
    "    # Split the output images into training (4), validation (1), and testing (1)\n",
    "    y_train = torch.tensor([y_images[0], y_images[1], y_images[3], y_images[5]])\n",
    "    y_val = torch.tensor(y_images[4]).unsqueeze(0)\n",
    "    y_test = torch.tensor(y_images[2]).unsqueeze(0)\n",
    "\n",
    "    # Create windows of patches for training, validation, and testing\n",
    "    x_train_patch, y_train_patch = create_patches_all(x_train, y_train)\n",
    "    x_val_patch, y_val_patch = create_patches_all(x_val, y_val)\n",
    "    x_test_patch, y_test_patch = create_patches_all(x_test, y_test)\n",
    "\n",
    "    return x_train_patch, y_train_patch, x_val_patch, y_val_patch, x_test_patch, y_test_patch\n",
    "\n",
    "\n",
    "\n",
    "def create_patches(image, stride=28, patch_height=224, patch_width=224):\n",
    "    \"\"\"\n",
    "    Create overlapping patches from an input image.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image as a NumPy array.\n",
    "        stride (int): Stride for patch extraction. Default is 28.\n",
    "        patch_height (int): Height of each patch. Default is 224.\n",
    "        patch_width (int): Width of each patch. Default is 224.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of patches extracted from the input image.\n",
    "\n",
    "\n",
    "    Example:\n",
    "        >>> patches = create_patches(image_array, stride=20, \n",
    "                                     patch_height=128, patch_width=128)\n",
    "    \"\"\"\n",
    "    # Convert to array \n",
    "    image = np.array(image)\n",
    "\n",
    "    # Extract overlapping patches\n",
    "    patches = view_as_windows(image, (patch_height, patch_width), step=stride)\n",
    "\n",
    "    # Reshape the patches to create a list of individual patches\n",
    "    patches = patches.reshape(-1, patch_height, patch_width)\n",
    "    \n",
    "    patches = np.expand_dims(patches, 1)\n",
    "    return patches\n",
    "\n",
    "\n",
    "def create_patches_all(x, y, stride=28, patch_height=224, patch_width=224):\n",
    "    \"\"\"\n",
    "    Create overlapping patches for a list of input images.\n",
    "\n",
    "    Parameters:\n",
    "        x (list): List of input images (NumPy arrays).\n",
    "        y (list): List of corresponding target images (NumPy arrays).\n",
    "        stride (int): Stride for patch extraction. Default is 28.\n",
    "        patch_height (int): Height of each patch. Default is 224.\n",
    "        patch_width (int): Width of each patch. Default is 224.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two tensors containing patches for input (x) and target (y) images.\n",
    "\n",
    "\n",
    "    Example:\n",
    "        >>> x_patches, y_patches = create_patches_all(x_images_list, \n",
    "                                                    y_images_list, \n",
    "                                                    stride=20, \n",
    "                                                    patch_height=128, \n",
    "                                                    patch_width=128)\n",
    "    \"\"\"\n",
    "\n",
    "    x_patch = []\n",
    "    y_patch = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        x_patch.extend(create_patches(x[i], stride, patch_height, patch_width))\n",
    "        y_patch.extend(create_patches(y[i], stride, patch_height, patch_width))\n",
    "\n",
    "    y_patch =  torch.tensor(y_patch, dtype=torch.float)\n",
    "    x_patch =  torch.tensor(x_patch, dtype=torch.float)\n",
    "    return x_patch, y_patch \n",
    "\n",
    "def get_training_augmentation():\n",
    "    \"\"\"\n",
    "    Defines a set of data augmentations for training images.\n",
    "    \n",
    "    Returns:\n",
    "        albumentations.Compose: Composition of image augmentations.\n",
    "\n",
    "    Example:\n",
    "        >>> augmentation = get_training_augmentation()\n",
    "        >>> augmented_image = augmentation(image=my_input_image, mask=my_mask_image)\n",
    "    \"\"\"\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        albu.RandomCrop(height=224, width=224, always_apply=True),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for handling input and target data with optional augmentation and preprocessing.\n",
    "\n",
    "    Parameters:\n",
    "        x_data (list or torch.Tensor): List or tensor containing input data.\n",
    "        y_data (list or torch.Tensor): List or tensor containing target data.\n",
    "        augmentation (albumentations.Compose, optional): Augmentation to be applied to the data.\n",
    "\n",
    "    Methods:\n",
    "        __len__: Returns the number of samples in the dataset.\n",
    "        __getitem__: Returns a sample from the dataset at the specified index.\n",
    "\n",
    "    Example:\n",
    "        >>> dataset = CustomDataset(x_data=my_input_data, y_data=my_target_data, augmentation=my_augmentation, preprocessing=my_preprocessing)\n",
    "        >>> sample = dataset[0]\n",
    "        >>> input_sample, target_sample = sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data, augmentation=None):\n",
    "        \"\"\"\n",
    "        Initializes the CustomDataset.\n",
    "\n",
    "        Args:\n",
    "            x_data (list or torch.Tensor): List or tensor containing input data.\n",
    "            y_data (list or torch.Tensor): List or tensor containing target data.\n",
    "            augmentation (albumentations.Compose, optional): Augmentation to be applied to the data.\n",
    "            preprocessing (function, optional): Preprocessing function to be applied to the data.\n",
    "        \"\"\"\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input and target samples.\n",
    "        \"\"\"\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "\n",
    "        # Apply augmentation if specified\n",
    "        if self.augmentation:\n",
    "            x = np.array(x).transpose(1, 2, 0)\n",
    "            y = np.array(y).transpose(1, 2, 0)\n",
    "            sample = self.augmentation(image=x, mask=y)\n",
    "            x, y = sample['image'], sample['mask']\n",
    "            x = torch.tensor(x.transpose(2, 0, 1))\n",
    "            y = torch.tensor(y.transpose(2, 0, 1))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def pad_image(image, block_size=224):\n",
    "    \"\"\"\n",
    "    Pads an image to make its dimensions divisible by a specified block size.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image to be padded.\n",
    "        block_size (int, optional): Size of the block. Defaults to 224.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the padded image and the padding dimensions.\n",
    "    \"\"\"\n",
    "    # Calculate the padding needed to make the dimensions divisible by block_size\n",
    "    height, width = image.shape\n",
    "    pad_height = (block_size - height % block_size) % block_size\n",
    "    pad_width = (block_size - width % block_size) % block_size\n",
    "\n",
    "    # Calculate the padding values for the top and bottom (ph) and left and right (pw) sides\n",
    "    ph = int(pad_height / 2)\n",
    "    pw = int(pad_width / 2)\n",
    "\n",
    "    # Pad the image with zeros if needed\n",
    "    padded_image = np.pad(image, ((ph, ph), (pw, pw)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Return the padded image and padding dimensions as a tuple\n",
    "    return padded_image, (ph, pw)\n",
    "\n",
    "\n",
    "def unpad_image(image, padding):\n",
    "    \"\"\"\n",
    "    Removes padding from an image based on the specified padding dimensions.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Padded image to be unpadded.\n",
    "        padding (tuple): Padding dimensions (ph, pw).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The unpadded image.\n",
    "    \"\"\"\n",
    "    # Extract padding dimensions\n",
    "    ph, pw = padding\n",
    "\n",
    "    # Remove the padding from the image\n",
    "    unpadded_image = image[ph:-ph, pw:-pw]\n",
    "\n",
    "    # Return the unpadded image\n",
    "    return unpadded_image\n",
    "\n",
    "\n",
    "def image_to_blocks(image):\n",
    "    \"\"\"\n",
    "    Divides an image into blocks of a desired size.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image to be divided into blocks.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the blocks as a PyTorch tensor and the padding dimensions.\n",
    "    \"\"\"\n",
    "    # Desired block size\n",
    "    block_size = 224\n",
    "\n",
    "    # Pad the image\n",
    "    padded_image, padding = pad_image(image, block_size)\n",
    "\n",
    "    # Divide the padded image into 224x224 blocks\n",
    "    blocks = view_as_blocks(padded_image, (block_size, block_size))\n",
    "    \n",
    "    # Return blocks as a PyTorch tensor and padding dimensions\n",
    "    return torch.tensor(blocks.astype(np.float32)), padding\n",
    "\n",
    "\n",
    "def blocks_to_image(blocks): \n",
    "    \"\"\"\n",
    "    Reconstructs an image from blocks.\n",
    "\n",
    "    Args:\n",
    "        blocks (numpy.ndarray): Blocks to be reconstructed into an image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The reconstructed image.\n",
    "    \"\"\"\n",
    "    # Concatenate blocks horizontally to form rows\n",
    "    rows = [np.hstack(row) for row in blocks]\n",
    "\n",
    "    # Stack rows vertically to reconstruct the image\n",
    "    reconstructed_image = np.vstack(rows)\n",
    "\n",
    "    # Return the reconstructed image\n",
    "    return reconstructed_image\n",
    "\n",
    "\n",
    "def train_single_models(targets):\n",
    "    \"\"\"\n",
    "    Trains a U-Net model for each target protein and evaluates its performance.\n",
    "\n",
    "    Args:\n",
    "        targets (list): List of target proteins.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create directories for saving model results\n",
    "    os.makedirs('single_model_results_unet_v2/trained_models', exist_ok=True)\n",
    "    os.makedirs('single_model_results_unet_v2/test_predictions', exist_ok=True)\n",
    "    os.makedirs('single_model_results_unet_v2/model_results', exist_ok=True)\n",
    "\n",
    "    for p, target in enumerate(targets):\n",
    "        print('\\n ---------------------------------------------')\n",
    "        print('Training model for ', target)\n",
    "\n",
    "        print('Preprocessing data ...')\n",
    "        # Preprocess data \n",
    "        x_train_patch, y_train_patch, x_val_patch, y_val_patch, x_test_patch, y_test_patch = preprocess_data(target)\n",
    "\n",
    "        # Load into dataloaders\n",
    "        custom_dataset = CustomDataset(x_train_patch, y_train_patch, augmentation=get_training_augmentation())\n",
    "        train_dataloader = DataLoader(custom_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "        custom_dataset = CustomDataset(x_val_patch, y_val_patch)\n",
    "        val_dataloader = DataLoader(custom_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "\n",
    "        custom_dataset = CustomDataset(x_test_patch, y_test_patch)\n",
    "        test_dataloader = DataLoader(custom_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "\n",
    "        print('Building model...')\n",
    "        # Build autoencoder  \n",
    "        ENCODER = 'resnet152'\n",
    "        ENCODER_WEIGHTS = 'imagenet'\n",
    "        ACTIVATION = 'sigmoid'  # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "        DEVICE = 'cuda'\n",
    "\n",
    "        model = smp.Unet(\n",
    "            encoder_name=ENCODER,\n",
    "            encoder_weights=ENCODER_WEIGHTS,\n",
    "            in_channels=1,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=1,\n",
    "            activation=ACTIVATION,\n",
    "        )\n",
    "\n",
    "        # Freeze encoder layers\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        loss = smp.utils.losses.DiceLoss()\n",
    "        metrics = [\n",
    "            smp.utils.metrics.IoU(threshold=0.5),\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam([\n",
    "            dict(params=model.parameters(), lr=0.0001),\n",
    "        ])\n",
    "\n",
    "        train_epoch = smp.utils.train.TrainEpoch(\n",
    "            model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            optimizer=optimizer,\n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        valid_epoch = smp.utils.train.ValidEpoch(\n",
    "            model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            device=DEVICE,\n",
    "            verbose=True,\n",
    "        )\n",
    "        max_score = 0\n",
    "        \n",
    "        # Train model \n",
    "        print('Training model...')\n",
    "        for i in range(0, 10):\n",
    "\n",
    "            print('\\nEpoch: {}'.format(i))\n",
    "            train_logs = train_epoch.run(train_dataloader)\n",
    "            valid_logs = valid_epoch.run(val_dataloader)\n",
    "\n",
    "            if max_score < valid_logs['iou_score']:\n",
    "                max_score = valid_logs['iou_score']\n",
    "                torch.save(model, 'single_model_results_unet_v2/trained_models/'+target+'_best_model.pth')\n",
    "                print('Model saved!')\n",
    "\n",
    "        # Load best model\n",
    "        best_model = torch.load('single_model_results_unet_v2/trained_models/'+target+'_best_model.pth')\n",
    "        best_model.eval()\n",
    "\n",
    "        # evaluate model on the test set\n",
    "        test_epoch = smp.utils.train.ValidEpoch(\n",
    "            model=best_model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "        logs = test_epoch.run(test_dataloader)\n",
    "\n",
    "        # Paths to data \n",
    "        x_dirs = glob('/home/ec2-user/SageMaker/x/*/*'+target+'*')\n",
    "        y_dirs = glob('/home/ec2-user/SageMaker/Threshold_images_TS/*/*'+target+'*')\n",
    "        \n",
    "        # Load test data and histogram match \n",
    "        matched_image = imread(x_dirs[0])\n",
    "        x_test = torch.tensor(min_max_normalize(exposure.match_histograms(imread(x_dirs[2]), matched_image))).unsqueeze(0)\n",
    "        y_test = torch.tensor(min_max_normalize(imread(y_dirs[2]))).unsqueeze(0)\n",
    "        \n",
    "        # Split test data into non-ovelapping windows/blocks\n",
    "        x_in, padding = image_to_blocks(x_test[0])\n",
    "        x_in = x_in.reshape(12, 1, 224, 224)\n",
    "        x_out = best_model.predict(x_in.cuda())\n",
    "\n",
    "        # Predict and reshape back into original dimensions \n",
    "        pred_img = x_out.squeeze(0).squeeze(0).cpu().numpy().round()  # Select those above 0.4\n",
    "        pred_img = pred_img.reshape(4, 3, 224, 224)\n",
    "        pr_mask_stacked = blocks_to_image(pred_img)\n",
    "        predicted_mask = unpad_image(pr_mask_stacked, padding)\n",
    "        \n",
    "        # Save results and measure performance \n",
    "        print('Testing model, saving results ... ')\n",
    "        fig, ax = plt.subplots(1, 3)\n",
    "        fig.suptitle(proteins[p])\n",
    "        ax[0].imshow(x_test[0])\n",
    "        ax[0].set_title('Raw')\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(y_test[0])\n",
    "        ax[1].set_title('Manual threshold')\n",
    "        ax[1].axis('off')\n",
    "        ax[2].imshow(predicted_mask)\n",
    "        ax[2].set_title('Predicted threshold')\n",
    "        ax[2].axis('off')\n",
    "        fig.savefig('single_model_results_unet_v2/test_predictions/'+target+'_prediction.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Test metrics\n",
    "        predicted_labels = predicted_mask.flatten()\n",
    "        true_labels = y_test[0].flatten()\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Save accuracy, precision, recall and F1 for each model \n",
    "        df = pd.DataFrame([[target, np.round(accuracy, 4), np.round(precision, 4), np.round(recall, 4), np.round(f1, 4)]],\n",
    "                          columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "        df.to_csv('single_model_results_unet_v2/model_results/'+target+'_metrics.csv')\n",
    "\n",
    "\n",
    "            \n",
    "def plot_images(image_list, rows, cols,savename=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Plot a grid of images from the given image list.\n",
    "    \n",
    "    Parameters:\n",
    "        image_list (list): List of images (e.g., NumPy arrays or matplotlib AxesImage objects).\n",
    "        rows (int): Number of rows in the grid.\n",
    "        cols (int): Number of columns in the grid.\n",
    "        figsize (tuple): Size of the figure. Default is (10, 10).\n",
    "    \"\"\"\n",
    "    # Create a new figure and set the size of the grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    \n",
    "    # Flatten the axes array to iterate over all subplots\n",
    "    flat_axes = axes.flatten()\n",
    "    \n",
    "    # Plot each image in the grid\n",
    "    for i, ax in enumerate(flat_axes):\n",
    "        if i < len(image_list):\n",
    "            ax.imshow(image_list[i], cmap='gray')\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    # Adjust layout and show the plot\n",
    "    plt.subplots_adjust(left=0.02, right=0.98, bottom=0.02, top=0.98, wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "    if savename != None: \n",
    "        fig.savefig(savename)       \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
